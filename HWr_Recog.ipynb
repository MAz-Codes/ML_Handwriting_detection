{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get into TensorFlow and train a model to predict the number from hand writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is: 2.13.0\n",
      "NumPy versio is: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "#this loads the dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version is:\", tf.__version__)\n",
    "print(\"NumPy versio is:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "# Loading the MNIST dataset. This dataset is divided into two parts:\n",
    "# 1. Training data (`x_train`, `y_train`): The model learns from this data.\n",
    "# 2. Test data (`x_test`, `y_test`): This data is used to evaluate how well the model has learned.\n",
    "# `x_train` and `x_test` are arrays of grayscale image data with shapes (num_samples, 28, 28).\n",
    "# `y_train` and `y_test` are arrays of digit labels with shapes (num_samples,).\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizing the image data from [0, 255] to [0, 1]. Normalization helps in faster convergence and-\n",
    "# -reduces the chance of getting stuck in local optima during training.\n",
    "# `x_train` and `x_test` are now arrays of floats with values ranging from 0 to 1.\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6636676   0.01171646  0.43151    -0.6944033   0.557099    0.0711689\n",
      "   0.41610742 -1.1918176   0.18610823 -0.1101667 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building a Sequential model using TensorFlow's Keras API. This model has four layers:\n",
    "# 1. A Flatten layer that transforms each 2D image into a 1D array.\n",
    "# 2. A Dense (fully connected) layer with 128 neurons and ReLU activation.\n",
    "# 3. A Dropout layer that randomly sets 20% of the input units to 0 during training,\n",
    "# which helps prevent overfitting.\n",
    "# 4. Another Dense layer with 10 neurons, corresponding to the 10 classes of digits\n",
    "# (0-9). This is the output layer.\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Making predictions using the untrained model. The model's weights are initially set randomly,\n",
    "# so these predictions will not be accurate until after the model is trained.\n",
    "# The prediction is made on the first image in the training set (`x_train[0]`). The output\n",
    "# `predictions` is an array of logits, one for each class.\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04997171 0.09818378 0.14940108 0.04845915 0.16939335 0.10419805\n",
      "  0.14711756 0.02946806 0.11688993 0.08691736]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Applying the softmax function to the model's output. Softmax converts the model's logits (raw predictions)\n",
    "# into probabilities by exponentiating each logit and then normalizing the results so they sum to 1.\n",
    "# The output is an array of probabilities that represent the model's confidence that the input\n",
    "# image belongs to each of the 10 classes of digits.\n",
    "probs = tf.nn.softmax(predictions).numpy()\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9962983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining the loss function for the model. The SparseCategoricalCrossentropy loss is used when\n",
    "# there are two or more label classes.\n",
    "# The `from_logits=True` argument means that the function should convert the logits into probabilities\n",
    "# (using softmax) before calculating the loss.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Calculating the loss of the model on the first training example. The loss is a measure of how far\n",
    "# the model's predictions are from the true labels.\n",
    "# A lower loss indicates that the model's predictions are closer to the true labels.\n",
    "# The output is a single floating-point number representing the loss.\n",
    "loss_value = loss_fn(y_train[1:2], predictions).numpy()\n",
    "\n",
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compiling the model with the Adam optimizer and the previously defined loss function.\n",
    "# The Adam optimizer is an algorithm for gradient-based optimization of stochastic objective functions.\n",
    "# It uses moving averages of the parameters (momentum) to take bigger steps, and it works well in\n",
    "# practice and requires little configuration.\n",
    "# The loss function (loss_fn) is used to calculate the distance between the model's predictions and\n",
    "# the actual values.\n",
    "# During training, the model will try to minimize this distance.\n",
    "# The 'accuracy' metric is used to measure the proportion of correct predictions out of all predictions.\n",
    "# This metric will be calculated and printed during training for us to monitor.\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2912 - accuracy: 0.9156\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.1442 - accuracy: 0.9568\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 2s 822us/step - loss: 0.1082 - accuracy: 0.9672\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 2s 812us/step - loss: 0.0887 - accuracy: 0.9726\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 2s 813us/step - loss: 0.0774 - accuracy: 0.9761\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 2s 839us/step - loss: 0.0671 - accuracy: 0.9784\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 2s 937us/step - loss: 0.0598 - accuracy: 0.9809\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 2s 969us/step - loss: 0.0546 - accuracy: 0.9828\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 2s 853us/step - loss: 0.0488 - accuracy: 0.9840\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 2s 818us/step - loss: 0.0469 - accuracy: 0.9840\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 2s 839us/step - loss: 0.0429 - accuracy: 0.9856\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 2s 844us/step - loss: 0.0400 - accuracy: 0.9871\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0362 - accuracy: 0.9879\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 2s 829us/step - loss: 0.0355 - accuracy: 0.9880\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 2s 928us/step - loss: 0.0334 - accuracy: 0.9884\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 2s 849us/step - loss: 0.0327 - accuracy: 0.9886\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 2s 865us/step - loss: 0.0310 - accuracy: 0.9897\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 2s 843us/step - loss: 0.0299 - accuracy: 0.9898\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.0259 - accuracy: 0.9908\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 2s 887us/step - loss: 0.0277 - accuracy: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28d102f90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training the model on the training data (`x_train` and `y_train`).\n",
    "# The model will learn to associate images and labels.\n",
    "# The training process will run for a fixed number of iterations through the dataset\n",
    "# (10 times, as specified by `epochs=10`).\n",
    "# Each iteration through the dataset is called an epoch.\n",
    "# During each epoch, the model's parameters (weights and biases) will be updated to minimize\n",
    "# the loss function.\n",
    "# The loss and accuracy metrics are calculated after each epoch and printed out for us to monitor\n",
    "# the training process.\n",
    "model.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0873 - accuracy: 0.9791 - 195ms/epoch - 623us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.087283194065094, 0.9790999889373779]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model's performance on the test data (`x_test` and `y_test`).\n",
    "# This gives us an unbiased estimate of the model's performance on new, unseen data.\n",
    "# The `verbose=2` argument controls the verbosity mode, 2 means that it will print one line per epoch.\n",
    "# The function will output the loss value and metrics values for the model in test mode.\n",
    "# In this case, it will return the loss and the accuracy of the model on the test data.\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining a new model (`probability_model`) that appends a softmax layer to the trained model.\n",
    "# The softmax function converts the model's logits (raw prediction values) into probabilities for each class.\n",
    "# This makes the model's predictions more interpretable.\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# Making predictions with the `probability_model` on the first two instances in the test set.\n",
    "# The output is a 2x10 array of probabilities. Each row corresponds to an instance, and each column\n",
    "# corresponds to a class.\n",
    "# The probabilities in a row sum to 1, and the highest probability in a row indicates the predicted\n",
    "# class for the corresponding instance.\n",
    "predictions = probability_model(x_test[:2])\n",
    "\n",
    "# Finding the predicted classes for the two instances by finding the index of the highest probability\n",
    "# in each row.\n",
    "# The output is an array of class labels.\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Printing the predicted classes.\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
